{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMk70/C9ucVcuRQVC0vLtW3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"40c3de039611410e9ff8b67af6569e4d":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_56b39f04dbf64942925f28d88aca5979","IPY_MODEL_ca1fbfb642f8434a8be32b7491445d1d","IPY_MODEL_0c74cd14547b4383b907580721061b06","IPY_MODEL_829b1fa381f945c9bf7e84b8bad884dc","IPY_MODEL_24a583354d324aceb3ca828e86be2f7b"],"layout":"IPY_MODEL_63643e5c454545f6a83baa45647da0a0"}},"56b39f04dbf64942925f28d88aca5979":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21e8a0ac3612424c8fb3c014761aed56","placeholder":"​","style":"IPY_MODEL_7d23b85931eb49fba7692e8dee5bcc98","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"ca1fbfb642f8434a8be32b7491445d1d":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_43739dbd462f453b861de6890e705673","placeholder":"​","style":"IPY_MODEL_b9de740b71d44f20a6612424133b04ab","value":""}},"0c74cd14547b4383b907580721061b06":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_e21afedecb274d2b96feed49fd5625cd","style":"IPY_MODEL_fbf34c5d2ea84e1fb8ce1a860c4a2f76","value":true}},"829b1fa381f945c9bf7e84b8bad884dc":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_53e22ea4e4744859abc35e8321a3e8ea","style":"IPY_MODEL_dc409c4768864b7382c1259dc98ff807","tooltip":""}},"24a583354d324aceb3ca828e86be2f7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8eeccd8028cb4f84943ed33bda882436","placeholder":"​","style":"IPY_MODEL_834d5059c95441c1b29552b9706bae58","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"63643e5c454545f6a83baa45647da0a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"21e8a0ac3612424c8fb3c014761aed56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d23b85931eb49fba7692e8dee5bcc98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43739dbd462f453b861de6890e705673":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9de740b71d44f20a6612424133b04ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e21afedecb274d2b96feed49fd5625cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbf34c5d2ea84e1fb8ce1a860c4a2f76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53e22ea4e4744859abc35e8321a3e8ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc409c4768864b7382c1259dc98ff807":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"8eeccd8028cb4f84943ed33bda882436":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"834d5059c95441c1b29552b9706bae58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l2LZN0Kw8izF","executionInfo":{"status":"ok","timestamp":1725986924257,"user_tz":-120,"elapsed":22076,"user":{"displayName":"Ziyi Wang","userId":"09196657380424983734"}},"outputId":"a3871092-50d1-4fe1-bf66-50560b7ced1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/dogvscat\n"]}],"source":["import os\n","import shutil\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/dogvscat"]},{"cell_type":"code","source":["!pip install transformers peft torch datasets scikit-learn\n"],"metadata":{"id":"eJhd2O2J83tN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1TQL7gPk9MEz","executionInfo":{"status":"ok","timestamp":1725986993337,"user_tz":-120,"elapsed":287,"user":{"displayName":"Ziyi Wang","userId":"09196657380424983734"}},"outputId":"ba69ee68-fd0b-481f-9b5c-a771db2d2636"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":[" binary_classification\t images.tar.gz\t\t     test_binary       train_multiclass\n"," dataset.ipynb\t\t multiclass_classification   test_multiclass   Untitled0.ipynb\n"," images\t\t\t resnet-E.ipynb（副本）      train_binary     'Vit Pet.ipynb'\n"]}]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331,"referenced_widgets":["40c3de039611410e9ff8b67af6569e4d","56b39f04dbf64942925f28d88aca5979","ca1fbfb642f8434a8be32b7491445d1d","0c74cd14547b4383b907580721061b06","829b1fa381f945c9bf7e84b8bad884dc","24a583354d324aceb3ca828e86be2f7b","63643e5c454545f6a83baa45647da0a0","21e8a0ac3612424c8fb3c014761aed56","7d23b85931eb49fba7692e8dee5bcc98","43739dbd462f453b861de6890e705673","b9de740b71d44f20a6612424133b04ab","e21afedecb274d2b96feed49fd5625cd","fbf34c5d2ea84e1fb8ce1a860c4a2f76","53e22ea4e4744859abc35e8321a3e8ea","dc409c4768864b7382c1259dc98ff807","8eeccd8028cb4f84943ed33bda882436","834d5059c95441c1b29552b9706bae58"]},"id":"1bTnzrP1DIkR","executionInfo":{"status":"ok","timestamp":1725986995028,"user_tz":-120,"elapsed":863,"user":{"displayName":"Ziyi Wang","userId":"09196657380424983734"}},"outputId":"94b2109c-5c68-456d-d944-a2fa35f804f3"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c3de039611410e9ff8b67af6569e4d"}},"metadata":{}}]},{"cell_type":"code","source":["import os\n","import time\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import ViTImageProcessor, AutoModelForImageClassification, AdamW\n","from peft import PeftConfig, PeftModel\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from tqdm import tqdm\n","\n","# Load Google Drive paths\n","train_multiclass_data_dir = \"./train_multiclass\"\n","test_multiclass_data_dir = \"./test_multiclass\"\n","\n","# Load the processor and model\n","processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n","repo_name = 'alanahmet/vit-base-patch16-224-finetuned-lora-oxfordPets'\n","\n","classes = os.listdir(train_multiclass_data_dir)\n","multiclass_label2id = {c: idx for idx, c in enumerate(classes)}\n","multiclass_id2label = {idx: c for idx, c in enumerate(classes)}\n","\n","# Load model configuration and inference model\n","config = PeftConfig.from_pretrained(repo_name)\n","model = AutoModelForImageClassification.from_pretrained(\n","    config.base_model_name_or_path,\n","    label2id=multiclass_label2id,\n","    id2label=multiclass_id2label,\n","    ignore_mismatched_sizes=True\n",")\n","inference_model = PeftModel.from_pretrained(model, repo_name)\n","\n","\n","# Custom Dataset class with limited data loading\n","class CustomDataset(Dataset):\n","    def __init__(self, data_dir, label2id, processor):\n","        self.data_dir = data_dir\n","        self.label2id = label2id\n","        self.processor = processor\n","        self.image_paths = []\n","        self.labels = []\n","        self.load_images_from_folder()\n","\n","    def load_images_from_folder(self):\n","        for class_name in os.listdir(self.data_dir):\n","            label = self.label2id[class_name]\n","            folder_path = os.path.join(self.data_dir, class_name)\n","            # Limit the number of images loaded for each class\n","            for idx, filename in enumerate(os.listdir(folder_path)):\n","                img_path = os.path.join(folder_path, filename)\n","                self.image_paths.append(img_path)\n","                self.labels.append(label)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        label = self.labels[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","        inputs = self.processor(image, return_tensors=\"pt\")\n","        return inputs[\"pixel_values\"].squeeze(0), label\n","\n","# Prepare training and testing datasets and dataloaders (with limited data)\n","train_dataset = CustomDataset(train_multiclass_data_dir, multiclass_label2id, processor)\n","test_dataset = CustomDataset(test_multiclass_data_dir, multiclass_label2id, processor)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=16)  # Smaller batch size for testing\n","test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=16)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ebSXJ_EWTgfx","executionInfo":{"status":"ok","timestamp":1725990551144,"user_tz":-120,"elapsed":2115,"user":{"displayName":"Ziyi Wang","userId":"09196657380424983734"}},"outputId":"ea2e6aaa-8ffa-4a68-d1be-daee27984a0d"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([37]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([37, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["# Training function with timing and logging\n","def train_model(model, dataloader, optimizer, loss_fn, epochs):\n","    model.train()\n","    epoch_losses = []  # To store loss for each epoch\n","    epoch_times = []  # To store time for each epoch\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        epoch_start_time = time.time()\n","\n","        for pixel_values, labels in tqdm(dataloader):\n","            pixel_values = pixel_values.to(\"cuda\")\n","            labels = labels.to(\"cuda\")\n","\n","            outputs = model(pixel_values=pixel_values)\n","            logits = outputs.logits\n","            loss = loss_fn(logits, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        epoch_end_time = time.time()\n","        avg_loss = total_loss / len(dataloader)\n","        epoch_time = epoch_end_time - epoch_start_time\n","\n","        epoch_losses.append(avg_loss)\n","        epoch_times.append(epoch_time)\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Time: {epoch_time:.2f} seconds\")\n","\n","    return epoch_losses, epoch_times\n","\n","# Evaluation function with timing and logging\n","def evaluate_model(model, dataloader):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    total_time = 0\n","\n","    with torch.no_grad():\n","        for pixel_values, labels in tqdm(dataloader):\n","            pixel_values = pixel_values.to(\"cuda\")\n","            labels = labels.to(\"cuda\")\n","\n","            start_time = time.time()\n","            outputs = model(pixel_values=pixel_values)\n","            logits = outputs.logits\n","            predicted_class_idx = logits.argmax(dim=-1)\n","            end_time = time.time()\n","\n","            total_time += (end_time - start_time)\n","\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted_class_idx.cpu().numpy())\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=1)\n","    avg_inference_time = total_time / len(dataloader)\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1-score: {f1:.4f}\")\n","    print(f\"Total inference time: {total_time:.2f} seconds\")\n","    print(f\"Average inference time per batch: {avg_inference_time:.4f} seconds\")\n","\n","    return accuracy, precision, recall, f1, total_time, avg_inference_time\n","\n","# Experiment function to run different configurations\n","def run_experiment(model, lr, epochs):\n","    # Copy base model and inference model\n","    model.to(\"cuda\")\n","\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    print(f\"Running experiment with lr={lr}, epochs={epochs}\")\n","\n","    # Train model and record losses and times\n","    epoch_losses, epoch_times = train_model(model, train_dataloader, optimizer, loss_fn, epochs)\n","\n","    # Evaluate model\n","    accuracy, precision, recall, f1, total_time, avg_inference_time = evaluate_model(model, test_dataloader)\n","\n","    return {\n","        \"learning_rate\": lr,\n","        \"epochs\": epochs,\n","        \"epoch_losses\": epoch_losses,\n","        \"epoch_times\": epoch_times,\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1_score\": f1,\n","        \"total_inference_time\": total_time,\n","        \"avg_inference_time_per_batch\": avg_inference_time\n","    }\n","\n","# Run multiple experiments with different configurations\n","experiments = [\n","    {\"lr\": 5e-5, \"epochs\": 10},\n","    {\"lr\": 1e-5, \"epochs\": 10},\n","    {\"lr\": 3e-5, \"epochs\": 10},\n","    {\"lr\": 5e-5, \"epochs\": 20},\n","    {\"lr\": 1e-5, \"epochs\": 20},\n","    {\"lr\": 3e-5, \"epochs\": 20}\n","]\n","\n","results = []\n","for experiment in experiments:\n","    inference_model = PeftModel.from_pretrained(model, repo_name)\n","    print(f\"Running experiment with lr={experiment['lr']}, epochs={experiment['epochs']}\")\n","    result = run_experiment(inference_model, experiment[\"lr\"], experiment[\"epochs\"])\n","    results.append(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A03jHhpmWa61","outputId":"dba31c1f-542e-4224-e300-22fcede0e644"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Running experiment with lr=5e-05, epochs=10\n","Running experiment with lr=5e-05, epochs=10\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/370 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","100%|█████████▉| 369/370 [01:28<00:00,  5.37it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","100%|██████████| 370/370 [01:28<00:00,  4.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 13.6932, Time: 88.84 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:31<00:00,  4.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10, Loss: 8.5618, Time: 91.72 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:24<00:00,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10, Loss: 3.7830, Time: 84.70 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:27<00:00,  4.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/10, Loss: 1.0009, Time: 87.07 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:31<00:00,  4.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/10, Loss: 0.3863, Time: 91.40 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:25<00:00,  4.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/10, Loss: 0.2504, Time: 85.63 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:25<00:00,  4.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/10, Loss: 0.1920, Time: 85.13 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:28<00:00,  4.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/10, Loss: 0.1584, Time: 88.38 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:25<00:00,  4.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/10, Loss: 0.1361, Time: 86.01 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:25<00:00,  4.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/10, Loss: 0.1200, Time: 85.75 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 93/93 [00:21<00:00,  4.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9675\n","Precision: 0.9686\n","Recall: 0.9675\n","F1-score: 0.9675\n","Total inference time: 4.48 seconds\n","Average inference time per batch: 0.0482 seconds\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Running experiment with lr=1e-05, epochs=10\n","Running experiment with lr=1e-05, epochs=10\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/370 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","100%|██████████| 370/370 [01:29<00:00,  4.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 15.7291, Time: 89.58 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:24<00:00,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10, Loss: 14.6974, Time: 84.59 seconds\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [01:25<00:00,  4.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10, Loss: 13.6670, Time: 85.31 seconds\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 52/370 [00:13<01:15,  4.23it/s]"]}]},{"cell_type":"code","source":["import os\n","import time\n","from PIL import Image\n","import torch\n","from transformers import ViTImageProcessor, AutoModelForImageClassification\n","from peft import PeftConfig, PeftModel\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from tqdm import tqdm\n","\n","# Load Google Drive paths\n","train_multiclass_data_dir = \"./train_multiclass\"\n","multiclass_data_dir = \"./test_multiclass\"\n","\n","# Load the processor and model\n","processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n","repo_name = 'alanahmet/vit-base-patch16-224-finetuned-lora-oxfordPets'\n","\n","# Load model configuration and inference model\n","config = PeftConfig.from_pretrained(repo_name)\n","model = AutoModelForImageClassification.from_pretrained(\n","    config.base_model_name_or_path,\n","    label2id=label2id,\n","    id2label=id2label,\n","    ignore_mismatched_sizes=True\n",")\n","inference_model = PeftModel.from_pretrained(model, repo_name)\n","\n","# Load a limited number of images for testing (use smaller batches)\n","def load_images_from_folder(folder, label):\n","    images = []\n","    labels = []\n","    for idx, filename in enumerate(os.listdir(folder)):\n","        img_path = os.path.join(folder, filename)\n","        try:\n","            image = Image.open(img_path).convert(\"RGB\")\n","            images.append(image)\n","            labels.append(label)\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","    return images, labels\n","\n","\n","# Multiclass Classification (All Classes)\n","test_multiclass_images = []\n","test_multiclass_labels = []\n","classes = os.listdir(multiclass_data_dir)\n","multiclass_label2id = {c: idx for idx, c in enumerate(classes)}\n","multiclass_id2label = {idx: c for idx, c in enumerate(classes)}\n","\n","# Load only 5 images per class for fast testing\n","for class_name in classes:\n","    print(f\"Loading images for class: {class_name}\")\n","    images, labels = load_images_from_folder(os.path.join(multiclass_data_dir, class_name), multiclass_label2id[class_name])\n","    test_multiclass_images.extend(images)\n","    test_multiclass_labels.extend(labels)\n","\n","# Function to perform inference and calculate metrics\n","def evaluate_model(images, true_labels, id2label, binary=False):\n","    y_true = []\n","    y_pred = []\n","    start_time = time.time()\n","\n","    for image, true_label in tqdm(zip(images, true_labels), total=len(images)):\n","        encoding = processor(image, return_tensors=\"pt\")\n","        with torch.no_grad():\n","            outputs = inference_model(**encoding)\n","            logits = outputs.logits\n","        predicted_class_idx = logits.argmax(-1).item()\n","        y_true.append(true_label)\n","        y_pred.append(predicted_class_idx)\n","\n","    end_time = time.time()\n","    avg_inference_time = (end_time - start_time) / len(images)\n","\n","    # Ensure binary classification is handled correctly\n","    unique_true_labels = set(y_true)\n","    unique_pred_labels = set(y_pred)\n","\n","    if binary and len(unique_true_labels) > 2:\n","        print(f\"Warning: More than two unique classes found in binary classification: {unique_true_labels}. Proceeding with evaluation.\")\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_true, y_pred)\n","\n","    if binary and len(unique_true_labels) <= 2:\n","        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=1)\n","    else:\n","        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=1)\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1-score: {f1:.4f}\")\n","    print(f\"Average inference time per image: {avg_inference_time:.4f} seconds\")\n","\n","    return accuracy, precision, recall, f1, avg_inference_time\n","\n","\n","# Evaluate on Binary Classification (Cats vs Dogs)\n","# print(\"\\nBinary Classification (Cats vs Dogs):\")\n","# evaluate_model(test_binary_images, test_binary_labels, binary_classes, binary=True)\n","\n","# Evaluate on Multiclass Classification (All Classes)\n","print(\"\\nMulticlass Classification (All Classes):\")\n","evaluate_model(test_multiclass_images, test_multiclass_labels, multiclass_id2label, binary=False)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CsdmT-8c9UQ8","executionInfo":{"status":"ok","timestamp":1725809911146,"user_tz":-120,"elapsed":1487629,"user":{"displayName":"Ziyi Wang","userId":"09196657380424983734"}},"outputId":"c5424a51-57ae-4efe-ff5f-af6f1ae40c76"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([37]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([37, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Loading images for class: pug\n","Loading images for class: american_bulldog\n","Loading images for class: yorkshire_terrier\n","Loading images for class: great_pyrenees\n","Loading images for class: shiba_inu\n","Loading images for class: Persian\n","Loading images for class: saint_bernard\n","Loading images for class: beagle\n","Loading images for class: Maine_Coon\n","Loading images for class: japanese_chin\n","Loading images for class: Bengal\n","Loading images for class: Sphynx\n","Loading images for class: english_cocker_spaniel\n","Loading images for class: newfoundland\n","Loading images for class: chihuahua\n","Loading images for class: english_setter\n","Loading images for class: miniature_pinscher\n","Loading images for class: Russian_Blue\n","Loading images for class: wheaten_terrier\n","Loading images for class: havanese\n","Loading images for class: american_pit_bull_terrier\n","Loading images for class: British_Shorthair\n","Loading images for class: Egyptian_Mau\n","Loading images for class: boxer\n","Loading images for class: leonberger\n","Loading images for class: keeshond\n","Loading images for class: pomeranian\n","Loading images for class: Ragdoll\n","Loading images for class: samoyed\n","Loading images for class: Abyssinian\n","Loading images for class: scottish_terrier\n","Loading images for class: Birman\n","Loading images for class: german_shorthaired\n","Loading images for class: Siamese\n","Loading images for class: Bombay\n","Loading images for class: basset_hound\n","Loading images for class: staffordshire_bull_terrier\n","\n","Multiclass Classification (All Classes):\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1479/1479 [19:41<00:00,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.0264\n","Precision: 0.0277\n","Recall: 0.0264\n","F1-score: 0.9736\n","Average inference time per image: 0.7988 seconds\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.02636916835699797,\n"," 0.027688451326703938,\n"," 0.02636916835699797,\n"," 0.9735965915986199,\n"," 0.7987600715681055)"]},"metadata":{},"execution_count":29}]}]}